{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 171 Problem Set 2\n",
    "# Due Monday, October 28, 2019 @ 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read *all* cells carefully and answer all parts (both text and missing code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter your information below:\n",
    "\n",
    "<div style=\"color: #000000;background-color: #EEEEFF\">\n",
    "    Your Name (submitter): Yaming Zhang<br>\n",
    "Your student ID (submitter):X674002\n",
    "    \n",
    "<hr>\n",
    "\n",
    "Collaborators, optional (they do *not* need to submit their own)\n",
    "\n",
    "Collaborator 1 name:Shiyi Zhang<br>\n",
    "Collaborator 1 student ID:X674358\n",
    "\n",
    "Collaborator 2 name:<br>\n",
    "Collaborator 2 student ID:\n",
    "\n",
    "(max of 2 collaborators)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this problem set, we will revisit the same movie review data from last time.  We will first try to predict the numeric score of the review (regression) using ridge regression (problems 1 and 2).  Then, we will try to just predict whether it is a good review or bad review (problems 3 and 4).\n",
    "\n",
    "The code below imports all allowed libraries and loads the data.  The variables loaded are as follows\n",
    "- Training data:\n",
    "    - `trainX` the data matrix, as is standard.  Each feature is a little different from last time.  The ith feature corresponds to the ith most common word across all reviews.  It is still related to the number of times the word is used in the review.  However, this time instead of bucketing this number into a category, we use the real value. Except, that instead of the raw count, we record the number of standard deviations this raw count is away from the mean raw count.  Why?  Well, we will cover that in week 9 or 10.  However, this makes things work better.  So, if the value is 0, then this review uses this word the average number of times.  If the value is +1, this review uses this word one standard deviation higher than average.  If -1, it uses it one standard deviation less than average.\n",
    "    - `trainYreg` the regression prediciton values.  We don't predict the raw rating (from 0 to 10), but rather the difference of this raw rating and 5.  So if the value in this vector is +3, that means the rating was an 8.  If the value is -4, the actual rating was a 1.\n",
    "    - `trainYclass` the classification prediction values.  These are +1 for positive reviews and -1 for negative reviews, same as last time.\n",
    "- Testing data:\n",
    "    - `testX` same as `trainX` but for the testing set\n",
    "    - `testYreg` same as `trainYreg` but for the training set\n",
    "    - `testYclass` same as `trainYclass` but for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "# load the data\n",
    "def loadsparsedata(fn):\n",
    "    \n",
    "    fp = open(fn,\"r\")\n",
    "    lines = fp.readlines()\n",
    "    maxf = 0;\n",
    "    for line in lines:\n",
    "        for i in line.split()[1::2]:\n",
    "            maxf = max(maxf,int(i))\n",
    "    \n",
    "    X = np.zeros((len(lines),maxf))\n",
    "    Y = np.zeros((len(lines)))\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        values = line.split()\n",
    "        Y[i] = int(values[0])\n",
    "        for j,v in zip(values[1::2],values[2::2]):\n",
    "            X[i,int(j)-1] = int(v)\n",
    "    \n",
    "    X = (X-X.mean(axis=0))/X.std(axis=0)\n",
    "    return X,Y\n",
    "\n",
    "def loadplusones(fn):\n",
    "    (X,Y) = loadsparsedata(fn)\n",
    "    X = np.column_stack((np.ones(X.shape[0]),X))\n",
    "    return X,Y\n",
    "\n",
    "(trainX,trainYreg) = loadplusones('sptrainreal.txt')\n",
    "(testX,testYreg) = loadplusones('sptestreal.txt')\n",
    "trainYreg = trainYreg - 5\n",
    "testYreg = testYreg - 5\n",
    "trainYclass = np.sign(trainYreg)\n",
    "testYclass = np.sign(testYreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "    <font size=+2>Part I: Ridge Regression</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 1:</font> <font size=+1>(3 points)</font>\n",
    "    \n",
    "Complete the training and testing functions below for ridge regression.\n",
    "    \n",
    "Do **not** penalize the initial weight (corresponding to the intercept term).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnridge(X,Y,lam):\n",
    "    # X is the data matrix of shape (m,n)\n",
    "    # Y is are the target values of shape (m,)\n",
    "    # lam is the value of lambda (careful, lambda is a reserved keyword in python)\n",
    "    # function should return w of shape (n,)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    penalty=np.eye(X.shape[1])\n",
    "    penalty[0][0]=0\n",
    "    w = np.linalg.inv(X.T@X+lam*penalty)@X.T@Y\n",
    "    return w\n",
    "    \n",
    "def predictridge(X,w):\n",
    "    # X is the (testing) data of shape (m,n)\n",
    "    # w are the weights learned in ridge regression\n",
    "    # function should return Y, the predicted values of shape (m,)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    Y=X@w\n",
    "    return Y\n",
    "    \n",
    "def testridge(X,Y,w):\n",
    "    # X and Y are the testing data\n",
    "    # w are the weights from ridge regression\n",
    "    # returns the mean squared error\n",
    "    Ydelta = Y - predictridge(X,w)\n",
    "    return (Ydelta*Ydelta).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.3516266355563165\n"
     ]
    }
   ],
   "source": [
    "print(testridge(testX,testYreg,learnridge(trainX,trainYreg,0.01)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 2:</font> <font size=+1>(5 points)</font>\n",
    "\n",
    "Use 3-fold cross validation to select the value of lambda for ridge regression, using `trainX` and `trainYreg`.  Plot the average across the three folds of the average squared error on the validation set as a function of lambda.  Use `plt.semilogx` for your plot (i.e., the horizontal axis for lambda should be on a log scale).  Use 10 values of lambda, arrange logarithmically evenly between $10^2$ and $10^5$.  See `np.logspace`.  If the number of data points does not divide by 3 evenly, just divide as evenly as possible.  `np.array_split` might help, but there are other ways.\n",
    "    \n",
    "Save the chosen value for lambda in `ridgelam`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "lam=np.logspace(2,5,10)\n",
    "minerr0=np.zeros((3,10))\n",
    "minerr1=np.zeros(3)\n",
    "minlam=np.zeros(3)\n",
    "for i in range (3):\n",
    "    minerr1[i]=float('inf')\n",
    "    Xv=np.array_split(trainX,3)[i]\n",
    "    Yv=np.array_split(trainYreg,3)[i]\n",
    "    if(i==0):\n",
    "        Xt=np.append(np.array_split(trainX,3)[1],np.array_split(trainX,3)[2],axis=0)\n",
    "        Yt=np.append(np.array_split(trainYreg,3)[1],np.array_split(trainYreg,3)[2],axis=0)\n",
    "    if(i==1):\n",
    "        Xt=np.append(np.array_split(trainX,3)[0],np.array_split(trainX,3)[2],axis=0)\n",
    "        Yt=np.append(np.array_split(trainYreg,3)[0],np.array_split(trainYreg,3)[2],axis=0)\n",
    "    else:\n",
    "        Xt=np.append(np.array_split(trainX,3)[0],np.array_split(trainX,3)[1],axis=0)\n",
    "        Yt=np.append(np.array_split(trainYreg,3)[0],np.array_split(trainYreg,3)[1],axis=0)\n",
    "    for j in range (10):\n",
    "        minerr0[i][j]=minerr0[i][j]+testridge(Xv,Yv,learnridge(Xt,Yt,lam[j]))\n",
    "        if(testridge(Xv,Yv,learnridge(Xt,Yt,lam[j]))<minerr1[i]):\n",
    "            minerr1[i]=testridge(Xv,Yv,learnridge(Xt,Yt,lam[j]))\n",
    "            minlam[i]=lam[j]\n",
    "ploterr=np.zeros(10)\n",
    "for j in range (10):\n",
    "    ploterr[j]=(minerr0[0][j]+minerr0[1][j]+minerr0[2][j])/3\n",
    "plt.semilogx(lam,ploterr)\n",
    "minerror=float('inf')\n",
    "for i in range (3):\n",
    "    if(minerr1[i]<minerror):\n",
    "        minerror=minerr1[i]\n",
    "        ridgelam=minlam[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "    <font size=+2>Part II: Logistic Regression</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function for logistic regression, $f(x) = \\sigma(x^\\top w)$, is\n",
    "\\begin{align*}\n",
    "l(y,\\hat{y}) &= -\\ln \\sigma(y\\hat{y}) \\\\\n",
    "\\text{and thus} \\\\\n",
    "L &= \\frac{1}{m}\\sum_{i=1}^m -\\ln \\sigma(y_i f(x_i))\n",
    "\\end{align*}\n",
    "In class, we derived that the resulting gradient was, therefore,\n",
    "\\begin{align*}\n",
    "\\nabla_w L &= \\frac{-1}{m}\\sum_{i=1}^m (1-p_i)y_ix_i \\\\\n",
    "\\text{where} \\\\\n",
    "p_i &= \\sigma(y_i w^\\top x_i)\n",
    "\\end{align*}\n",
    "And so the update rule for $w$ is\n",
    "\\begin{align*}\n",
    "w &\\leftarrow w + \\eta \\frac{1}{m}\\sum_{i=1}^m (1-p_i)y_ix_i \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 3:</font> <font size=+1>(3 points)</font>\n",
    "\n",
    "Modify the total loss function, $L$, to include a regularization term with strength $\\lambda/m$ that penalizes the sum of the squares of the weights.\n",
    "\n",
    "***Write the new loss function.  Derive the gradient descent rule for this new loss function.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new loss function is:\n",
    "\\begin{align*}\n",
    "L &= \\frac{1}{m}\\sum_{i=1}^m -\\ln \\sigma(y_i f(x_i))+\\frac{\\lambda}{m}\\sum_{j=1}^n \\omega^{2}_{j}\n",
    "\\end{align*}\n",
    "The resulting gradient is:\n",
    "\\begin{align*}\n",
    "\\nabla_w L &= \\frac{2\\lambda}{m}\\omega-\\frac{1}{m}\\sum_{i=1}^m (1-p_i)y_ix_i \\\\\n",
    "\\text{where} \\\\\n",
    "p_i &= \\sigma(y_i w^\\top x_i)\n",
    "\\end{align*}\n",
    "The update rule is:\n",
    "\\begin{align*}\n",
    "w &\\leftarrow w - \\eta(\\frac{2\\lambda}{m}\\omega-\\frac{1}{m}\\sum_{i=1}^m (1-p_i)y_ix_i) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 4:</font> <font size=+1>(7 points)</font>\n",
    "    \n",
    "Complete the training and testing functions below for logistic regression.  We will use a constant step size of 0.2.  Picking a good step size is tricky, but this one should work well for this assignment.  Start $w$ at 0.  Use **batch** (or standard) gradient descent.  Stochastic gradient descent is harder to tell whether it is converging.  Stop when the squared magnitude of the gradient vector is less that $10^{-5}$.  Do not penalize the initial weight, corresponding to the intercept term.\n",
    "    \n",
    "A few hints:\n",
    "- This function will need to be written without loops (except for the loop over iterations of gradient descent) to be fast enough for the next question.\n",
    "- You can use `print` to output debugging information (or even use pyplot to plot things!).  The line `clear_output(wait=True)` will clear the output, in case you don't want the cell's output to extend too far during debugging.  (please remove debugging output when submitting)\n",
    "- To check to see if it is working, you should look that the gradient is getting smaller, but (more importantly) that the objective function (the loss) is getting smaller.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnlogreg(X,Y,lam):\n",
    "    # X is the data matrix of shape (m,n)\n",
    "    # Y is are the target labels (+1,-1) of shape (m,)\n",
    "    # lam is the value of lambda (careful, lambda is a reserved keyword in python)\n",
    "    # function should return w of shape (n,)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    w=np.zeros(X.shape[1])\n",
    "    def sigmoid(x):\n",
    "        sig = 1 / (1 + np.exp(-x))\n",
    "        return sig\n",
    "    \n",
    "    def get_grad(X,Y,w):\n",
    "        grad=2*lam/X.shape[0]*w\n",
    "        for i in range (X.shape[0]):\n",
    "            grad=grad+(1/X.shape[0])*(sigmoid(Y[i]*w.T@X[i])-1)*Y[i]*X[i]\n",
    "        return grad\n",
    "    \n",
    "    while get_grad(X,Y,w).dot(get_grad(X,Y,w).T)>1e-3:\n",
    "        w=w-0.2*get_grad(X,Y,w)\n",
    "    return w\n",
    "                \n",
    "def predictlogreg(X,w):\n",
    "    # X is the (testing) data of shape (m,n)\n",
    "    # w are the weights learned in ridge regression\n",
    "    # function should return Y, the predicted values of shape (m,) (all values either +1 or -1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    predYbool=(X@w)>=0\n",
    "    predY=np.ones(predYbool.shape)\n",
    "    predY[predYbool==False]=-1\n",
    "    return predY\n",
    "    \n",
    "def testlogreg(X,Y,w):\n",
    "    # X and Y are the testing data\n",
    "    # w are the weights from ridge regression\n",
    "    # returns the mean squared error\n",
    "    Ypred = np.sign(predictlogreg(X,w)) ## should be +1/-1, but incase they are not\n",
    "    return (Ypred!=np.sign(Y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12388\n"
     ]
    }
   ],
   "source": [
    "print(testlogreg(trainX,trainYclass,learnlogreg(trainX,trainYclass,400)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 5:</font> <font size=+1>(4 points)</font>\n",
    "\n",
    "Use 3-fold cross validation to select the value of lambda for logistic regression, using `trainX` and `trainYclass`.  Plot the average across the three folds of the average squared error on the validation set as a function of lambda.  Use `plt.semilogx` for your plot (i.e., the horizontal axis for lambda should be on a log scale).  Use 10 values of lambda, arrange logarithmically evenly between $10^0$ and $10^4$.  See `np.logspace`. \n",
    "    \n",
    "Save the chosen value for lambda in `logreglam`\n",
    "    \n",
    "This part takes about 12 minutes, in my solutions.  This is a long time to wait for a debug cycle.  To debug your code, use fewer lambda values until you are sure your code is correct.  Better still, test learnlogreg separately until you are sure it is working.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-09e621daae30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mminerr0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminerr0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtestlogreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearnlogreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestlogreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearnlogreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mminerr1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mminerr1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestlogreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearnlogreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mminlam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-bda2aaeb883f>\u001b[0m in \u001b[0;36mlearnlogreg\u001b[0;34m(X, Y, lam)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mget_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-bda2aaeb883f>\u001b[0m in \u001b[0;36mget_grad\u001b[0;34m(X, Y, w)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "lam=np.logspace(0,4,3)\n",
    "minerr0=np.zeros((3,10))\n",
    "minerr1=np.zeros(3)\n",
    "minlam=np.zeros(3)\n",
    "for i in range (3):\n",
    "    minerr1[i]=float('inf')\n",
    "    Xv=np.array_split(trainX,3)[i]\n",
    "    Yv=np.array_split(trainYclass,3)[i]\n",
    "    if(i==0):\n",
    "        Xt=np.append(np.array_split(trainX,3)[1],np.array_split(trainX,3)[2],axis=0)\n",
    "        Yt=np.append(np.array_split(trainYclass,3)[1],np.array_split(trainYclass,3)[2],axis=0)\n",
    "    if(i==1):\n",
    "        Xt=np.append(np.array_split(trainX,3)[0],np.array_split(trainX,3)[2],axis=0)\n",
    "        Yt=np.append(np.array_split(trainYclass,3)[0],np.array_split(trainYclass,3)[2],axis=0)\n",
    "    else:\n",
    "        Xt=np.append(np.array_split(trainX,3)[0],np.array_split(trainX,3)[1],axis=0)\n",
    "        Yt=np.append(np.array_split(trainYclass,3)[0],np.array_split(trainYclass,3)[1],axis=0)\n",
    "    for j in range (3):\n",
    "        minerr0[i][j]=minerr0[i][j]+testlogreg(Xv,Yv,learnlogreg(Xt,Yt,lam[j]))\n",
    "        if(testlogreg(Xv,Yv,learnlogreg(Xt,Yt,lam[j]))<minerr1[i]):\n",
    "            minerr1[i]=testlogreg(Xv,Yv,learnlogreg(Xt,Yt,lam[j]))\n",
    "            minlam[i]=lam[j]\n",
    "ploterr=np.zeros(3)\n",
    "for j in range (3):\n",
    "    ploterr[j]=(minerr0[0][j]+minerr0[1][j]+minerr0[2][j])/3\n",
    "plt.semilogx(lam,ploterr)\n",
    "minerror=float('inf')\n",
    "for i in range (3):\n",
    "    if(minerr1[i]<minerror):\n",
    "        minerror=minerr1[i]\n",
    "        logreglam=minlam[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFEEFF\">\n",
    "    <font size=+2>Part III: Testing</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "The code below retrains ridge regression and logistic regression using the fond values of lambda, above.  It then reports the average error for each on the **testing** data.\n",
    "    \n",
    "Perhaps more interestingly, it also reports the error rate if the ridge regression method is used to predict whether the review is positive.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wridge = learnridge(trainX,trainYreg,ridgelam)\n",
    "wlogreg = learnlogreg(trainX,trainYclass,logreglam)\n",
    "\n",
    "ridgemse = testridge(testX,testYreg,wridge)\n",
    "logregerrrate = testlogreg(testX,testYclass,wlogreg)\n",
    "ridgeerrrate = testlogreg(testX,testYclass,wridge)\n",
    "\n",
    "print(\"mean squared error for ridge rgression = %f\" % ridgemse)\n",
    "print(\"error rate for logistic regression = %f\" % logregerrrate)\n",
    "print(\"error rate for ridge regression = %f\" % ridgeerrrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #000000;background-color: #FFFFEE\">\n",
    "    <font size=+2>Question 6:</font> <font size=+1>(3 points)</font>\n",
    "Given the results above, would you use ridge regression or logistic regression for this problem?  **Explain why.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS 171 Python",
   "language": "python",
   "name": "cs171-cpu-limitation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
